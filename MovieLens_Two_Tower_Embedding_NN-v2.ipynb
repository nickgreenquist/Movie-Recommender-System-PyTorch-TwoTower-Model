{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH1_445JoNNz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if torch will use Apple Silicon GPU\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_data_dir = \"ml-32m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbUl4oLiocXu"
   },
   "outputs": [],
   "source": [
    "num_ratings_to_read = 35_000_000\n",
    "\n",
    "df_ratings = pd.read_csv(movielens_data_dir + '/ratings.csv', nrows=num_ratings_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JwOzHRikpFxr",
    "outputId": "ecb25413-62c3-491f-9e9a-befb031d3b4d"
   },
   "outputs": [],
   "source": [
    "len(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuVToGGopH9z"
   },
   "outputs": [],
   "source": [
    "# clean the ratings data\n",
    "df_ratings = df_ratings.dropna()\n",
    "df_ratings['movieId'] = df_ratings['movieId'].astype(int, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QXNXWJio1oCB",
    "outputId": "d98aaf38-0584-496a-fa86-88f5e3d56fa8"
   },
   "outputs": [],
   "source": [
    "df_ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtiZsZrXpdAH"
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv(movielens_data_dir + '/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "poniXr1fpg7R",
    "outputId": "c0415585-ec62-4265-818b-6861db1b0b2e"
   },
   "outputs": [],
   "source": [
    "df_movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.read_csv(movielens_data_dir + '/tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags['tag'] = df_tags['tag'].str.lower()\n",
    "df_tags['tag'] = df_tags['tag'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_tags.groupby('tag').size().reset_index(name='count')\n",
    "df_tags_sorted_counts = counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "MIN_NUM_TAGS = 1000\n",
    "df_tags_final_counts = df_tags_sorted_counts[df_tags_sorted_counts['count'] > MIN_NUM_TAGS]  \n",
    "df_tags_final_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final tags we want to use as movie features\n",
    "final_movie_tags = set(df_tags_final_counts['tag'].tolist())\n",
    "len(final_movie_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsEUjaTXtHV5"
   },
   "source": [
    "# Movie Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMJt7sFtAXmD",
    "outputId": "ab43ff72-cecc-436f-f5c6-c2fecf012825"
   },
   "outputs": [],
   "source": [
    "# let's only work with movies with enough ratings.\n",
    "\n",
    "min_ratings_per_movie = 3_000\n",
    "\n",
    "# get the number of ratings per movie\n",
    "df_movies_to_num_ratings = df_ratings.groupby('movieId', as_index=False)['rating'].count()\n",
    "print(\"total movies in corpus: \", len(df_movies_to_num_ratings))\n",
    "\n",
    "df_movies_to_num_ratings = df_movies_to_num_ratings.sort_values(by=['rating'], ascending=False)\n",
    "df_movies_to_num_ratings = df_movies_to_num_ratings[df_movies_to_num_ratings['rating'] > min_ratings_per_movie]\n",
    "print(\"movies with enough ratings: \", len(df_movies_to_num_ratings))\n",
    "\n",
    "# get list of the top movies by number of ratings.\n",
    "top_movies = df_movies_to_num_ratings.movieId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTBR8snNEFvD"
   },
   "outputs": [],
   "source": [
    "# keep a map of movieId to number of ratings.\n",
    "movieId_to_num_ratings = {}\n",
    "movieId_list = df_movies_to_num_ratings.movieId.tolist()\n",
    "rating_list = df_movies_to_num_ratings.rating.tolist()\n",
    "for i in range(len(movieId_list)):\n",
    "  movieId_to_num_ratings[movieId_list[i]] = rating_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkXt5NOPqFqI"
   },
   "outputs": [],
   "source": [
    "# map movieId to title\n",
    "movieId_to_title = {}\n",
    "title_to_movieId = {}\n",
    "\n",
    "# map moveiId to year\n",
    "movieId_to_year = {}\n",
    "\n",
    "movieId_list = df_movies.movieId.tolist()\n",
    "title_list = df_movies.title.tolist()\n",
    "\n",
    "for i in range(len(movieId_list)):\n",
    "  movieId = movieId_list[i]\n",
    "  title = title_list[i]\n",
    "\n",
    "  movieId_to_title[movieId] = title\n",
    "  title_to_movieId[title] = movieId\n",
    "\n",
    "  match = re.search(r\"\\(\\d+\\)\\s*$\", title)\n",
    "  if match:\n",
    "    year = title[match.start()+1:match.end()-1]\n",
    "    movieId_to_year[movieId] = year\n",
    "  else:\n",
    "    movieId_to_year[movieId] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of the number of unique year values we see in movieId_to_year\n",
    "year_to_num_movies = {}\n",
    "for movieId,year in movieId_to_year.items():\n",
    "  if year not in year_to_num_movies:\n",
    "    year_to_num_movies[year] = 0\n",
    "  year_to_num_movies[year] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_CQZfpeqYJ8",
    "outputId": "728347cc-0a2c-49a6-a28b-4c28c96f1b1a"
   },
   "outputs": [],
   "source": [
    "# print the top movies\n",
    "for movieId in top_movies[0:10]:\n",
    "  print(movieId, movieId_to_title[movieId], movieId_to_num_ratings[movieId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-PkcA_pq67x"
   },
   "outputs": [],
   "source": [
    "# map movieId to list of genres for that movie\n",
    "genres = set()\n",
    "movieId_to_genres = {}\n",
    "\n",
    "movieId_list = df_movies.movieId.tolist()\n",
    "genre_list = df_movies.genres.tolist()\n",
    "\n",
    "for i in range(len(movieId_list)):\n",
    "  movieId = movieId_list[i]\n",
    "  if movieId not in top_movies:\n",
    "    continue\n",
    "\n",
    "  movieId_to_genres[movieId] = set()\n",
    "\n",
    "  for genre in genre_list[i].split('|'):\n",
    "    genres.add(genre)\n",
    "    movieId_to_genres[movieId].add(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5L0uRnlfrWAr",
    "outputId": "e9869df8-31ea-4828-db38-ff1a21c29239"
   },
   "outputs": [],
   "source": [
    "movieId_to_genres[title_to_movieId['Matrix, The (1999)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map movieId to the counts of tags for that movie\n",
    "movieId_to_tag_to_count = {}\n",
    "\n",
    "movieId_list = df_tags.movieId.tolist()\n",
    "tag_list = df_tags.tag.tolist()\n",
    "\n",
    "for i in range(len(movieId_list)):\n",
    "  movieId = movieId_list[i]\n",
    "  if movieId not in top_movies:\n",
    "    continue\n",
    "      \n",
    "  if movieId not in movieId_to_tag_to_count:\n",
    "    movieId_to_tag_to_count[movieId] = {}\n",
    "\n",
    "  tag = tag_list[i]\n",
    "  if tag not in final_movie_tags:\n",
    "    continue\n",
    "  if tag not in movieId_to_tag_to_count[movieId]:\n",
    "    movieId_to_tag_to_count[movieId][tag] = 0\n",
    "\n",
    "  movieId_to_tag_to_count[movieId][tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_to_tag_to_count[title_to_movieId['Matrix, The (1999)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2erACCTmDWOk"
   },
   "outputs": [],
   "source": [
    "# for every movie, get the avg rating\n",
    "df_movies_to_avg_rating = df_ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "\n",
    "movieId_to_avg_rating = {}\n",
    "\n",
    "movieId_list = df_movies_to_avg_rating.movieId.tolist()\n",
    "rating_list = df_movies_to_avg_rating.rating.tolist()\n",
    "for i in range(len(movieId_list)):\n",
    "  if movieId_list[i] not in top_movies: continue\n",
    "  movieId_to_avg_rating[movieId_list[i]] = rating_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlXL2NVrtag9"
   },
   "outputs": [],
   "source": [
    "# build ITEM movieId embedding mapping\n",
    "item_emb_movieId_to_i = {s:i for i,s in enumerate(top_movies)}\n",
    "item_emb_i_to_movieId = {i:s for s,i in item_emb_movieId_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rs8YTabXtp0H"
   },
   "outputs": [],
   "source": [
    "# build ITEM genre feature context\n",
    "genre_to_i = {s:i for i,s in enumerate(genres)}\n",
    "i_to_genre = {i:s for s,i in genre_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ITEM year embedding mapping\n",
    "year_to_i = {s:i for i,s in enumerate(year_to_num_movies.keys())}\n",
    "i_to_year = {i:s for s,i in year_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ITEM tag feature context\n",
    "tag_to_i = {s:i for i,s in enumerate(final_movie_tags)}\n",
    "i_to_tag = {i:s for s,i in tag_to_i.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiIp4GmVtKWR"
   },
   "source": [
    "# User Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyQWy0jylw1t"
   },
   "outputs": [],
   "source": [
    "# every user will have a feature context that will mostly be their watch history.\n",
    "# instead of using every movie in the corpus, we can use a smaller subset.\n",
    "# this helps with memory issues.\n",
    "num_movies_for_user_context = 250\n",
    "user_context_movies = top_movies[:num_movies_for_user_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJt4iYZGik-U"
   },
   "outputs": [],
   "source": [
    "df_ratings_final = df_ratings[df_ratings.movieId.isin(top_movies)]\n",
    "df_ratings_final = df_ratings_final.sort_values(['userId', 'timestamp'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP8o3iQDTs7M"
   },
   "outputs": [],
   "source": [
    "# aggregate dataframe down into one row per user and list of their movies and ratings.\n",
    "df_ratings_final = df_ratings_final.groupby('userId').agg(\n",
    "    {'movieId': lambda x: list(x),\n",
    "     'rating': lambda y: list(y),\n",
    "     'timestamp': lambda z: list(z)\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "BFt2zuQCTuss",
    "outputId": "1615e932-275e-488a-9ac7-a3a6423bea54"
   },
   "outputs": [],
   "source": [
    "df_ratings_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUrl8Vzsu_h-"
   },
   "outputs": [],
   "source": [
    "# build the USER context\n",
    "user_context_size = len(user_context_movies) + len(genres)\n",
    "\n",
    "user_context_movieId_to_i = {s:i for i,s in enumerate(list(user_context_movies))}\n",
    "user_context_i_to_movieId = {i:s for s,i in user_context_movieId_to_i.items()}\n",
    "\n",
    "user_context_genre_avg_rating_to_i = {s:i+len(user_context_movies) for i,s in enumerate(list(genres))}\n",
    "user_context_i_to_genre_avg_rating = {i:s for s,i in user_context_genre_avg_rating_to_i.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqs8Tf10v2qH"
   },
   "source": [
    "# Generate Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TfqvZbvESXz"
   },
   "outputs": [],
   "source": [
    "# simulate training examples by masking out some of the user's watched movies from their context, and using them as labels.\n",
    "# we do not want the 'movie to predict' in their watch history, as we are trying to simulate the following:\n",
    "# given the user's watch history, what would they rate this new movie?\n",
    "# NOTE: this is not the same as a train/test split. This is just simulating how training examples would look like on a movie platform.\n",
    "\n",
    "percent_ratings_as_watch_history = 0.9\n",
    "\n",
    "min_ratings_per_user = 20 # ignore users with too few movie watches\n",
    "max_ratings_per_user = 500 # ignore users with way too many movie watches\n",
    "too_few_ratings = 0\n",
    "too_many_ratings = 0\n",
    "\n",
    "user_to_movie_to_rating_WATCH_HISTORY = {}\n",
    "user_to_movie_to_rating_LABEL = {}\n",
    "user_to_movie_to_timestamp_LABEL = {}\n",
    "\n",
    "# loop over each column as this is much, much faster than going row by row.\n",
    "user_list = df_ratings_final['userId'].tolist()\n",
    "movieId_list_list = df_ratings_final['movieId'].tolist()\n",
    "rating_list_list = df_ratings_final['rating'].tolist()\n",
    "timestamp_list_list = df_ratings_final['timestamp'].tolist()\n",
    "\n",
    "for i in range(len(user_list)):\n",
    "  userId = user_list[i]\n",
    "  movieId_list = movieId_list_list[i]\n",
    "  rating_list = rating_list_list[i]\n",
    "  timestamp_list = timestamp_list_list[i]\n",
    "\n",
    "  num_rated_movies = len(movieId_list)\n",
    "\n",
    "  # ignore users with too few or too many ratings.\n",
    "  if num_rated_movies < min_ratings_per_user:\n",
    "    too_few_ratings += 1\n",
    "    continue\n",
    "  if num_rated_movies > max_ratings_per_user:\n",
    "    too_many_ratings += 1\n",
    "    continue\n",
    "\n",
    "  # set up training example maps.\n",
    "  user_to_movie_to_rating_WATCH_HISTORY[userId] = {}\n",
    "  user_to_movie_to_rating_LABEL[userId] = {}\n",
    "  user_to_movie_to_timestamp_LABEL[userId] = {}\n",
    "\n",
    "  # put earlier watched movies into user's watch history (features) and leave later watched movies as labels to predict.\n",
    "  rated_movies = list(zip(movieId_list, rating_list, timestamp_list))\n",
    "  for movieId,rating,timestamp in rated_movies[:int(num_rated_movies * percent_ratings_as_watch_history)]:\n",
    "    user_to_movie_to_rating_WATCH_HISTORY[userId][movieId] = rating\n",
    "  for movieId,rating,timestamp in rated_movies[int(num_rated_movies * percent_ratings_as_watch_history):]:\n",
    "    user_to_movie_to_rating_LABEL[userId][movieId] = rating\n",
    "    user_to_movie_to_timestamp_LABEL[userId][movieId] = timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iy-5m8DhMaLT",
    "outputId": "86e13964-1ac5-4e28-bfbc-63b46c65c5bf"
   },
   "outputs": [],
   "source": [
    "len(user_list), len(user_to_movie_to_rating_WATCH_HISTORY.keys()), too_few_ratings, too_many_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOB_rm7Rxi_n"
   },
   "outputs": [],
   "source": [
    "# for every user, get their avg rating.\n",
    "# this will help us debias each user's rating.\n",
    "user_to_avg_rating = {}\n",
    "\n",
    "# NOTE: only use ratings from their synthetic watch history.\n",
    "for user in user_to_movie_to_rating_WATCH_HISTORY.keys():\n",
    "  user_to_avg_rating[user] = 0\n",
    "  for movieId in user_to_movie_to_rating_WATCH_HISTORY[user].keys():\n",
    "    user_to_avg_rating[user] += user_to_movie_to_rating_WATCH_HISTORY[user][movieId]\n",
    "\n",
    "  user_to_avg_rating[user] /= len(user_to_movie_to_rating_WATCH_HISTORY[user].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-9pIqURyEWU",
    "outputId": "12a9285f-3e5a-4080-a51d-b5ed9c699718"
   },
   "outputs": [],
   "source": [
    "user_to_avg_rating[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHc_u8YVyGUs"
   },
   "outputs": [],
   "source": [
    "# for every user, get the avg rating for every genre\n",
    "user_to_genre_to_stat = {}\n",
    "\n",
    "# NOTE: only use ratings from their synthetic watch history.\n",
    "for user in user_to_movie_to_rating_WATCH_HISTORY.keys():\n",
    "  user_to_genre_to_stat[user] = {}\n",
    "  for movieId in user_to_movie_to_rating_WATCH_HISTORY[user].keys():\n",
    "    for genre in movieId_to_genres[movieId]:\n",
    "      if genre not in user_to_genre_to_stat[user]:\n",
    "        user_to_genre_to_stat[user][genre] = {\n",
    "            'NUM_RATINGS': 0,\n",
    "            'SUM_RATINGS': 0,\n",
    "        }\n",
    "\n",
    "      user_to_genre_to_stat[user][genre]['NUM_RATINGS'] += 1\n",
    "      user_to_genre_to_stat[user][genre]['SUM_RATINGS'] += user_to_movie_to_rating_WATCH_HISTORY[user][movieId]\n",
    "\n",
    "for user in user_to_genre_to_stat.keys():\n",
    "  for genre in user_to_genre_to_stat[user].keys():\n",
    "    num_ratings = user_to_genre_to_stat[user][genre]['NUM_RATINGS']\n",
    "    sum_ratings = user_to_genre_to_stat[user][genre]['SUM_RATINGS']\n",
    "    user_to_genre_to_stat[user][genre]['AVG_RATING'] = sum_ratings / num_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "582v8d0Sy91s"
   },
   "outputs": [],
   "source": [
    "# for every user, create the training example user context vector\n",
    "# 0:num_user_context_movies -> user's watch history\n",
    "# num_user_context_movies:num_user_context_movies+num_genres -> user's genre avg rating (debiased using user's avg rating)\n",
    "user_to_context = {}\n",
    "for user in user_to_movie_to_rating_WATCH_HISTORY.keys():\n",
    "  context = [0.0] * user_context_size\n",
    "\n",
    "  for movieId in user_to_movie_to_rating_WATCH_HISTORY[user].keys():\n",
    "    if movieId in user_context_movies:\n",
    "      # note, we debias the rating so if the rating is under the user's avg rating,\n",
    "      # it will hopefully count as negative strength for predicting similar movies.\n",
    "      # vice-versa for a rating above the user's average.\n",
    "      context[user_context_movieId_to_i[movieId]] = float(user_to_movie_to_rating_WATCH_HISTORY[user][movieId] - user_to_avg_rating[user])\n",
    "\n",
    "  for genre in user_to_genre_to_stat[user].keys():\n",
    "    # add the user's avg rating for this genre debiased using their actual avg rating from all movies\n",
    "    context[user_context_genre_avg_rating_to_i[genre]] = float(user_to_genre_to_stat[user][genre]['AVG_RATING'] - user_to_avg_rating[user])\n",
    "\n",
    "  user_to_context[user] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNLFC8wx0Gt2"
   },
   "outputs": [],
   "source": [
    "# for every movie, create a training example feature GENRE context vector lookup\n",
    "# it will contain the movie's genres.\n",
    "movieId_to_genre_context = {}\n",
    "for movieId in top_movies:\n",
    "  genre_context = [0.0] * len(genres)\n",
    "\n",
    "  for genre in movieId_to_genres[movieId]:\n",
    "    genre_context[genre_to_i[genre]] = float(1.0)\n",
    "\n",
    "  movieId_to_genre_context[movieId] = genre_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every movie, create a training example feature TAG context vector lookup\n",
    "# it will contain the movie's tags.\n",
    "movieId_to_tag_context = {}\n",
    "for movieId in top_movies:\n",
    "  tag_context = [0.0] * len(final_movie_tags)\n",
    "\n",
    "  for tag,count in movieId_to_tag_to_count[movieId].items():\n",
    "    tag_context[tag_to_i[tag]] = float(count)\n",
    "\n",
    "  movieId_to_tag_context[movieId] = tag_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_lJar1sJE48"
   },
   "source": [
    "# Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the buckets for our timestamps so we can use them as categorical features\n",
    "timestamp_num_bins = 1500\n",
    "timestamp_bins = torch.tensor(np.linspace(df_ratings['timestamp'].min(), df_ratings['timestamp'].max(), timestamp_num_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeZR-nqC0nyt"
   },
   "outputs": [],
   "source": [
    "# Build the final Dataset\n",
    "def build_dataset(users):\n",
    "  # the user context (i.e. the watch history and genre affinities)\n",
    "  X = []\n",
    "\n",
    "  # the timestamp the user watched this movie (the one we will learn to predict rating for)\n",
    "  timestamp = []\n",
    "\n",
    "  # the movieID for the movie we will predict rating for.\n",
    "  # used to lookup the movie embedding to feed into the NN item tower.\n",
    "  target_movieId = []\n",
    "\n",
    "  # the feature GENRE context of the movie we will predict the rating for.\n",
    "  # will also feed into it's own embedding and will be stacked with the embedding above.\n",
    "  target_movieId_genre_context = []\n",
    "\n",
    "  # the feature TAG context of the movie we will predict the rating for.\n",
    "  # will also feed into it's own embedding and will be stacked with the embedding above.\n",
    "  target_movieId_tag_context = []\n",
    "\n",
    "  # the year of the movie we will predict the rating for.\n",
    "  # used to lookup a year embedding to feed into the NN item tower.\n",
    "  target_movieId_year = []\n",
    "\n",
    "  # the predicted rating\n",
    "  Y = []\n",
    "\n",
    "  # create training examples, one for each movie the user has that we want as a label.\n",
    "  for user in users:\n",
    "    for movieId in user_to_movie_to_rating_LABEL[user].keys():\n",
    "      X.append(user_to_context[user])\n",
    "\n",
    "      timestamp.append(user_to_movie_to_timestamp_LABEL[user][movieId])\n",
    "\n",
    "      target_movieId.append(item_emb_movieId_to_i[movieId])\n",
    "\n",
    "      target_movieId_genre_context.append(movieId_to_genre_context[movieId])\n",
    "\n",
    "      target_movieId_tag_context.append(movieId_to_tag_context[movieId])\n",
    "\n",
    "      target_movieId_year.append(year_to_i[movieId_to_year[movieId]])\n",
    "\n",
    "      # remember to debias the user rating so we can learn to predict if user\n",
    "      # like/dislike a movie based on their features and the movie features.\n",
    "      Y.append(float(user_to_movie_to_rating_LABEL[user][movieId] - user_to_avg_rating[user]))\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  target_movieId = torch.tensor(target_movieId)\n",
    "  target_movieId_genre_context = torch.tensor(target_movieId_genre_context)\n",
    "  target_movieId_tag_context = torch.tensor(target_movieId_tag_context)\n",
    "  target_movieId_year = torch.tensor(target_movieId_year)\n",
    "\n",
    "  # Bucketize the timestamps\n",
    "  timestamp = torch.bucketize(torch.tensor(timestamp), timestamp_bins, right=False)\n",
    "\n",
    "  return X,timestamp,Y,target_movieId,target_movieId_genre_context,target_movieId_tag_context,target_movieId_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UL41mDO2am8",
    "outputId": "35df6222-232a-4f76-fda5-4e290104c006"
   },
   "outputs": [],
   "source": [
    "# user users with enough ratings to predict to be useful for model learning.\n",
    "final_users = []\n",
    "\n",
    "for user in user_to_movie_to_rating_LABEL.keys():\n",
    "  num_ratings = len(user_to_movie_to_rating_LABEL[user])\n",
    "\n",
    "  # having at least 2 watched movies in the user's LABEL split means they have watched at least 20 movies\n",
    "  # 18 of these movies are used for the user's watch history\n",
    "  if num_ratings >= 2 and num_ratings < 500:\n",
    "    final_users.append(user)\n",
    "\n",
    "len(final_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH7iFOJULzi7"
   },
   "outputs": [],
   "source": [
    "# split users into train and validation users\n",
    "percent_users_train = 0.9\n",
    "\n",
    "random.shuffle(final_users)\n",
    "\n",
    "train_users = final_users[:int(len(final_users) * percent_users_train)]\n",
    "validation_users = final_users[int(len(final_users) * percent_users_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOf31QdW2pIW"
   },
   "outputs": [],
   "source": [
    "X_train, timestamp_train, Y_train, target_movieId_train, target_movieId_genre_context_train, target_movieId_tag_context_train, target_movieId_year_train = build_dataset(train_users)\n",
    "X_val, timestamp_val, Y_val, target_movieId_val, target_movieId_genre_context_val, target_movieId_tag_context_val, target_movieId_year_val = build_dataset(validation_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUtSY4e32uLo",
    "outputId": "a4baa290-3e5c-498b-a5cd-f8c0e52083cc"
   },
   "outputs": [],
   "source": [
    "print(\"train: \", \n",
    "      X_train.shape,\n",
    "      timestamp_train.shape,\n",
    "      Y_train.shape,\n",
    "      target_movieId_train.shape,\n",
    "      target_movieId_genre_context_train.shape,\n",
    "      target_movieId_tag_context_train.shape,\n",
    "      target_movieId_year_train.shape)\n",
    "print(\"val: \",\n",
    "      X_val.shape,\n",
    "      timestamp_val.shape,\n",
    "      Y_val.shape,\n",
    "      target_movieId_val.shape,\n",
    "      target_movieId_genre_context_val.shape,\n",
    "      target_movieId_tag_context_val.shape,\n",
    "      target_movieId_year_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nE_-j5B3WVM"
   },
   "source": [
    "# Build our Two Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "user_features   -> u_W1\n",
    "                        \\         \n",
    "                         --> stack\n",
    "                        /          \\\n",
    "timestamp_emb   -> t_W1             \\\n",
    "                                     \\\n",
    "                                      \\\n",
    "                                       --> dot_product(user, item) --> prediction\n",
    "                                      /\n",
    "                                     /\n",
    "movie_genres    -> g_W1             /\n",
    "                                   /\n",
    "movie_tags      -> t_W1           /\n",
    "                        ---> stack\n",
    "movie_year_emb  -> y_W1\n",
    "                    \n",
    "movie_id_emb    -> e_W1\n",
    "'''\n",
    "\n",
    "class MovieRecommender(nn.Module):\n",
    "    def __init__(self,\n",
    "                 genres_len,\n",
    "                 tags_len,\n",
    "                 top_movies_len,\n",
    "                 all_years_len,\n",
    "                 user_context_size,\n",
    "                 timestamp_num_bins,\n",
    "                 item_genre_embedding_size=10,\n",
    "                 item_tag_embedding_size=10,\n",
    "                 item_movieId_embedding_size=40,\n",
    "                 item_year_embedding_size=10,\n",
    "                 user_feature_embedding_size=60,\n",
    "                 timestamp_feature_embedding_size=10,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the MovieRecommender model.\n",
    "\n",
    "        Args:\n",
    "            genres_len (int): Number of unique genres (dimension of movie genre features).\n",
    "            tags_len (int): Number of unique tags (dimension of movie tag features).\n",
    "            top_movies_len (int): Total number of unique movie IDs in the lookup table.\n",
    "            all_years_len (int): Number of unique movie release years.\n",
    "            user_context_size (int): Dimension of user context features.\n",
    "            item_genre_embedding_size (int): Desired embedding size for item genre features.\n",
    "            item_tag_embedding_size (int): Desired embedding size for item tag features.\n",
    "            item_movieId_embedding_size (int): Desired embedding size for movie IDs.\n",
    "            item_year_embedding_size (int): Desired embedding size for the movie year.\n",
    "            user_feature_embedding_size (int): Desired embedding size for user features.\n",
    "            timestamp_feature_embedding_size (int): Desired embedding size for the timestamp features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.item_genre_tower = nn.Sequential(\n",
    "            nn.Linear(genres_len, item_genre_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.item_tag_tower = nn.Sequential(\n",
    "            nn.Linear(tags_len, item_tag_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.item_embedding_lookup = nn.Embedding(top_movies_len, item_movieId_embedding_size)\n",
    "        self.item_embedding_tower = nn.Sequential(\n",
    "            nn.Linear(item_movieId_embedding_size, item_movieId_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.year_embedding_lookup = nn.Embedding(all_years_len, item_year_embedding_size)\n",
    "        self.year_embedding_tower = nn.Sequential(\n",
    "            nn.Linear(item_year_embedding_size, item_year_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.user_feature_tower = nn.Sequential(\n",
    "            nn.Linear(user_context_size, user_feature_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.timestamp_embedding_lookup = nn.Embedding(timestamp_num_bins, timestamp_feature_embedding_size)\n",
    "        self.timestamp_embedding_tower = nn.Sequential(\n",
    "            nn.Linear(timestamp_feature_embedding_size, timestamp_feature_embedding_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Dimension check:\n",
    "        # user+timestamp == item_embedding+year_embedding+item_genre+item_tag\n",
    "        expected_combined_item_embedding_size = item_genre_embedding_size + item_tag_embedding_size + item_movieId_embedding_size + item_year_embedding_size\n",
    "        expected_combined_user_embedding_size = user_feature_embedding_size + timestamp_feature_embedding_size\n",
    "        if expected_combined_user_embedding_size != expected_combined_item_embedding_size:\n",
    "            raise ValueError(\n",
    "                f\"User+timestamp embedding size ({expected_combined_user_embedding_size}) must match \"\n",
    "                f\"combined item embedding size ({expected_combined_item_embedding_size}) \"\n",
    "                f\"for the dot product operation. Please adjust `user_feature_embedding_size`.\"\n",
    "            )\n",
    "\n",
    "        # Apply custom weight initialization to all applicable layers\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Applies a small scale to initial weights for Linear and Embedding layers.\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Using Xavier uniform initialization which is good for Tanh activation\n",
    "            # The gain parameter scales the initialization.\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=0.01)\n",
    "            if module.bias is not None:\n",
    "                # Initialize biases to zero\n",
    "                torch.nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # For embedding layers, also use Xavier uniform initialization\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=0.01)\n",
    "\n",
    "    def forward(self, user_contexts, timestamps, movie_genres, movie_tags, years, target_movieId):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_contexts (torch.Tensor):  Batch of user context features.\n",
    "                                           Shape: (batch_size, user_context_size)\n",
    "            timestamps (torch.Tensor):     Batch of timestamp features.\n",
    "                                           Shape: (batch_size, timestamp_feature_embedding_size)\n",
    "            movie_genres (torch.Tensor):   Batch of target movie genre features.\n",
    "                                           Shape: (batch_size, genres_len)\n",
    "            movie_tags (torch.Tensor):     Batch of target movie tag features.\n",
    "                                           Shape: (batch_size, tags_len)\n",
    "            years (torch.Tensor):          Batch of target movie years.\n",
    "                                           Shape: (batch_size, all_years_len)\n",
    "            target_movieId (torch.Tensor): Batch of target movie ID indices.\n",
    "                                           Shape: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Forward pass through the USER tower\n",
    "        user_embedding = self.user_feature_tower(user_contexts) # Shape: (batch_size, user_feature_embedding_size)\n",
    "\n",
    "        # Lookup the TIMESTAMP embedding and pass through its non-linear layer\n",
    "        timestamp_embedding_hidden = self.timestamp_embedding_tower(self.timestamp_embedding_lookup(timestamps)) # Shape: (batch_size, timestamp_feature_embedding_size)\n",
    "\n",
    "        # Concatenate the USER and TIMESTAMP embeddings together\n",
    "        # `torch.cat` combines tensors along a given dimension.\n",
    "        # `dim=1` means concatenate along the feature dimension for each sample in the batch.\n",
    "        user_embedding_combined = torch.cat((user_embedding, timestamp_embedding_hidden), dim=1)\n",
    "        # Resulting shape: (batch_size, user_feature_embedding_size + timestamp_feature_embedding_size)\n",
    "\n",
    "        # Forward pass through the ITEM movie genre tower\n",
    "        item_genre_embedding = self.item_genre_tower(movie_genres) # Shape: (batch_size, item_genre_embedding_size)\n",
    "\n",
    "        # Forward pass through the ITEM movie tag tower\n",
    "        item_tag_embedding = self.item_tag_tower(movie_tags) # Shape: (batch_size, item_tag_embedding_size)\n",
    "\n",
    "        # Lookup the ITEM movieId embedding and pass through its non-linear layer\n",
    "        item_embedding_hidden = self.item_embedding_tower(self.item_embedding_lookup(target_movieId)) # Shape: (batch_size, item_movieId_embedding_size)\n",
    "\n",
    "        # Lookup the ITEM year embedding and pass through its non-linear layer\n",
    "        year_embedding_hidden = self.year_embedding_tower(self.year_embedding_lookup(years)) # Shape: (batch_size, item_year_embedding_size)\n",
    "\n",
    "        # Concatenate the two ITEM embeddings together\n",
    "        item_embedding_combined = torch.cat((item_genre_embedding, item_tag_embedding, item_embedding_hidden, year_embedding_hidden), dim=1)\n",
    "        # Resulting shape: (batch_size, item_genre_embedding_size + item_tag_embedding_size + item_movieId_embedding_size + item_year_embedding_size)\n",
    "\n",
    "        # The final prediction is the dot product of the user+timestamp embedding and the combined item embedding.\n",
    "        # `torch.einsum('ij, ij -> i', A, B)` performs element-wise multiplication\n",
    "        # and then sums along the 'j' dimension for each 'i'.\n",
    "        # This is equivalent to `(user_embedding * item_embedding_combined).sum(dim=1)`.\n",
    "        preds = torch.einsum('ij, ij -> i', user_embedding_combined, item_embedding_combined)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lNS2VzCbw_8"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "\n",
    "# Define model hyper-parameters\n",
    "item_genre_embedding_size = 10\n",
    "item_tag_embedding_size = 40\n",
    "item_movieId_embedding_size = 40\n",
    "item_year_embedding_size = 10\n",
    "\n",
    "# User embedding size + timestamp embedding size must match the sum of item_feature_embedding_size + item_movieId_embedding_size + item_year_embedding_size\n",
    "user_feature_embedding_size = 90\n",
    "timestamp_feature_embedding_size = 10\n",
    "\n",
    "# Instantiate the MovieRecommender model\n",
    "model = MovieRecommender(\n",
    "    genres_len=len(genres),\n",
    "    tags_len=len(final_movie_tags),\n",
    "    top_movies_len=len(top_movies),\n",
    "    all_years_len=len(year_to_num_movies),\n",
    "    user_context_size=user_context_size,\n",
    "    timestamp_num_bins=timestamp_num_bins,\n",
    "    item_genre_embedding_size=item_genre_embedding_size,\n",
    "    item_tag_embedding_size=item_tag_embedding_size,\n",
    "    item_movieId_embedding_size=item_movieId_embedding_size,\n",
    "    user_feature_embedding_size=user_feature_embedding_size,\n",
    "    timestamp_feature_embedding_size=timestamp_feature_embedding_size\n",
    ")\n",
    "\n",
    "# Print the total number of trainable parameters in the model\n",
    "print(f\"Number of trainable parameters: {sum(p.nelement() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    \n",
    "    # Move the model to the GPU\n",
    "    model.to(mps_device)\n",
    "    \n",
    "    # move validation data to MPS\n",
    "    X_val = X_val.to(mps_device)\n",
    "    timestamp_val = timestamp_val.to(mps_device)\n",
    "    target_movieId_genre_context_val = target_movieId_genre_context_val.to(mps_device)\n",
    "    target_movieId_tag_context_val = target_movieId_tag_context_val.to(mps_device)\n",
    "    target_movieId_year_val = target_movieId_year_val.to(mps_device)\n",
    "    target_movieId_val = target_movieId_val.to(mps_device)\n",
    "    Y_val = Y_val.to(mps_device)\n",
    "    \n",
    "    # move training data to MPS\n",
    "    X_train = X_train.to(mps_device)\n",
    "    timestamp_train = timestamp_train.to(mps_device)\n",
    "    target_movieId_genre_context_train = target_movieId_genre_context_train.to(mps_device)\n",
    "    target_movieId_tag_context_train = target_movieId_tag_context_train.to(mps_device)\n",
    "    target_movieId_year_train = target_movieId_year_train.to(mps_device)\n",
    "    target_movieId_train = target_movieId_train.to(mps_device)\n",
    "    Y_train = Y_train.to(mps_device)\n",
    "else:\n",
    "    # move validation data to CPU\n",
    "    X_val = X_val.cpu()\n",
    "    timestamp_val = timestamp_val.cpu()\n",
    "    target_movieId_genre_context_val = target_movieId_genre_context_val.cpu()\n",
    "    target_movieId_tag_context_val = target_movieId_tag_context_val.cpu()\n",
    "    target_movieId_year_val = target_movieId_year_val.cpu()\n",
    "    target_movieId_val = target_movieId_val.cpu()\n",
    "    Y_val = Y_val.cpu()\n",
    "    \n",
    "    # move training data to CPU\n",
    "    X_train = X_train.cpu()\n",
    "    timestamp_train = timestamp_train.cpu()\n",
    "    target_movieId_genre_context_train = target_movieId_genre_context_train.cpu()\n",
    "    target_movieId_tag_context_train = target_movieId_tag_context_train.cpu()\n",
    "    target_movieId_year_train = target_movieId_year_train.cpu()\n",
    "    target_movieId_train = target_movieId_train.cpu()\n",
    "    Y_train = Y_train.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function (Mean Squared Error Loss for regression)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Set the optimizer (Stochastic Gradient Descent)\n",
    "# It will manage the updates to all parameters in `model.parameters()`\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Training configuration\n",
    "minibatch_size = 64\n",
    "loss_train = [] # To store training loss for each step\n",
    "loss_val = []   # To store validation loss for full validation runs\n",
    "\n",
    "log_every = 10_000      # How often to perform a full validation run and log\n",
    "training_steps = 150_000 # Total training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting training loop...\")\n",
    "\n",
    "start = time.time()\n",
    "for i in range(training_steps):\n",
    "    is_full_val_run = False\n",
    "    if i % log_every == 0:\n",
    "        is_full_val_run = True\n",
    "\n",
    "    # Select data for the current step (training minibatch or full validation set)\n",
    "    if is_full_val_run:\n",
    "        # Use full validation set for evaluation\n",
    "        user_contexts_batch = X_val\n",
    "        timestamps_batch = timestamp_val\n",
    "        movie_genres_batch = target_movieId_genre_context_val\n",
    "        movie_tags_batch = target_movieId_tag_context_val\n",
    "        movie_years_batch = target_movieId_year_val\n",
    "        target_movieId_batch = target_movieId_val\n",
    "        Y_batch = Y_val\n",
    "\n",
    "        # Set model to evaluation mode (e.g., disables dropout if present)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad(): # Disable gradient calculations for validation\n",
    "            preds = model(user_contexts_batch, timestamps_batch, movie_genres_batch, movie_tags_batch, movie_years_batch, target_movieId_batch)\n",
    "            output = loss_fn(preds, Y_batch)\n",
    "\n",
    "        # Store validation loss for this batch\n",
    "        loss_val.append(output.item()) # Store validation loss\n",
    "    else:\n",
    "        # Construct a random minibatch from training data\n",
    "        ix = torch.randint(0, X_train.shape[0], (minibatch_size,))\n",
    "        user_contexts_batch = X_train[ix]\n",
    "        timestamps_batch = timestamp_train[ix]\n",
    "        movie_genres_batch = target_movieId_genre_context_train[ix]\n",
    "        movie_tags_batch = target_movieId_tag_context_train[ix]\n",
    "        movie_years_batch = target_movieId_year_train[ix]\n",
    "        target_movieId_batch = target_movieId_train[ix]\n",
    "        Y_batch = Y_train[ix]      \n",
    "\n",
    "        # Set model to training mode (e.g., enables dropout)\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(user_contexts_batch, timestamps_batch, movie_genres_batch, movie_tags_batch, movie_years_batch, target_movieId_batch)\n",
    "        output = loss_fn(preds, Y_batch)\n",
    "\n",
    "        # Backpropagation and update weights\n",
    "        optimizer.zero_grad() # Clear gradients from the previous step\n",
    "        output.backward()     # Compute gradients for all parameters\n",
    "        optimizer.step()      # Update model parameters using the computed gradients\n",
    "\n",
    "        # Store training loss for this batch\n",
    "        loss_train.append(output.item())\n",
    "\n",
    "    # Logging\n",
    "    if is_full_val_run:\n",
    "        # print how long in between each full val run\n",
    "        end = time.time()\n",
    "        length = end - start\n",
    "        print(\"Total time: \", length, \"seconds\")\n",
    "        start = time.time()\n",
    "        \n",
    "        # Calculate average training loss over the last `log_every` batches\n",
    "        if i >= log_every:\n",
    "            avg_train_loss_last_batches = np.mean(loss_train[i-log_every:i])\n",
    "        else:\n",
    "            # If not enough training batches collected yet, use the current validation loss\n",
    "            # (or you could skip logging train loss for the very first validation run)\n",
    "            avg_train_loss_last_batches = output.item()\n",
    "        print(f\"[TRAIN] i: {i:06d} | avg_loss (last {log_every} batches): {avg_train_loss_last_batches:.4f}\")\n",
    "        print(f\"[VAL]   i: {i:06d} | loss: {output.item():.4f}\\n\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "P8xcagsxeZf_",
    "outputId": "947332e3-e17e-4d7a-a31f-ce17c5250534"
   },
   "outputs": [],
   "source": [
    "loss_train_bucket_means = []\n",
    "for i in range(0, len(loss_train), log_every):\n",
    "  loss_train_bucket_means.append(np.mean(loss_train[i:i+log_every]))\n",
    "\n",
    "plt.plot([i*1000 for i in range(len(loss_train_bucket_means))], loss_train_bucket_means)\n",
    "plt.plot([i*1000 for i in range(1, len(loss_val))], loss_val[1:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F1xNlzXe4gi"
   },
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'saved_models/20250602.pth'\n",
    "\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F1xNlzXe4gi"
   },
   "source": [
    "# Actually Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovieRecommender(\n",
    "    genres_len=len(genres),\n",
    "    tags_len=len(final_movie_tags),\n",
    "    top_movies_len=len(top_movies),\n",
    "    all_years_len=len(year_to_num_movies),\n",
    "    user_context_size=user_context_size,\n",
    "    timestamp_num_bins=timestamp_num_bins,\n",
    "    item_genre_embedding_size=item_genre_embedding_size,\n",
    "    item_tag_embedding_size=item_tag_embedding_size,\n",
    "    item_movieId_embedding_size=item_movieId_embedding_size,\n",
    "    user_feature_embedding_size=user_feature_embedding_size,\n",
    "    timestamp_feature_embedding_size=timestamp_feature_embedding_size\n",
    ")\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJsFMEk8e6SZ"
   },
   "outputs": [],
   "source": [
    "# for every movie, save all its embeddings\n",
    "movieId_to_embedding = {}\n",
    "\n",
    "ITEM_EMBEDDING_LOOKUP = model.item_embedding_lookup.weight\n",
    "YEAR_EMBEDDING_LOOKUP = model.year_embedding_lookup.weight\n",
    "\n",
    "for movieId in top_movies:\n",
    "  movieId_to_embedding[movieId] = {}\n",
    "\n",
    "  item_embedding = ITEM_EMBEDDING_LOOKUP[torch.tensor([item_emb_movieId_to_i[movieId]])]\n",
    "  movieId_to_embedding[movieId]['MOVIEID_EMBEDDING'] = model.item_embedding_tower(item_embedding)\n",
    "\n",
    "  year_embedding = YEAR_EMBEDDING_LOOKUP[torch.tensor([year_to_i[movieId_to_year[movieId]]])]\n",
    "  movieId_to_embedding[movieId]['MOVIE_YEAR_EMBEDDING'] = model.year_embedding_tower(year_embedding)\n",
    "\n",
    "  movieId_to_embedding[movieId]['MOVIE_GENRE_EMBEDDING'] = model.item_genre_tower(torch.tensor([movieId_to_genre_context[movieId]]))\n",
    "  movieId_to_embedding[movieId]['MOVIE_TAG_EMBEDDING'] = model.item_tag_tower(torch.tensor([movieId_to_tag_context[movieId]]))\n",
    "\n",
    "  # compute the combined (concat) item/movie embedding\n",
    "  item_id_emb = movieId_to_embedding[movieId]['MOVIEID_EMBEDDING']\n",
    "  item_genre_emb = movieId_to_embedding[movieId]['MOVIE_GENRE_EMBEDDING']\n",
    "  item_tag_emb = movieId_to_embedding[movieId]['MOVIE_TAG_EMBEDDING']\n",
    "  iteam_year_emb = movieId_to_embedding[movieId]['MOVIE_YEAR_EMBEDDING']\n",
    "  movieId_to_embedding[movieId]['MOVIE_EMBEDDING_COMBINED'] = torch.cat((item_genre_emb, item_tag_emb, item_id_emb, iteam_year_emb), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVd99qEGfOwh",
    "outputId": "6f647549-05fd-4dca-c3a6-438bf45a5e47"
   },
   "outputs": [],
   "source": [
    "for emb_type in movieId_to_embedding[5952].keys():\n",
    "  print(movieId_to_embedding[5952][emb_type].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAxhh6sRyEDY"
   },
   "source": [
    "### Viewing Movies in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6z8ccMyyGTi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for movieId in top_movies[0:25]:\n",
    "  i = item_emb_movieId_to_i[movieId]\n",
    "  plt.scatter(ITEM_EMBEDDING_LOOKUP[i,0].data, ITEM_EMBEDDING_LOOKUP[i,1].data, s=200)\n",
    "  plt.text(ITEM_EMBEDDING_LOOKUP[i,0].item(), ITEM_EMBEDDING_LOOKUP[i,1].item(), movieId_to_title[movieId][0:20], ha=\"center\", va=\"center\", color='black')\n",
    "plt.grid('minor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDjCGr5Lhu-K"
   },
   "source": [
    "### Finding Most Similar Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tbu2Y9rggFaY"
   },
   "outputs": [],
   "source": [
    "# for every movie, and for every embedding type, find the similary to all other embeddings\n",
    "# NOTE: can be slow\n",
    "movieId_to_emb_type_to_similarities = {}\n",
    "\n",
    "for movieId in top_movies:\n",
    "  movieId_to_emb_type_to_similarities[movieId] = {}\n",
    "\n",
    "  # for emb_type in movieId_to_embedding[movieId].keys():\n",
    "  for emb_type in ['MOVIE_EMBEDDING_COMBINED']:\n",
    "    emb_to_target_to_dist = {}\n",
    "    for target_id in top_movies:\n",
    "      src = movieId_to_embedding[movieId][emb_type].view(-1)\n",
    "      target = movieId_to_embedding[target_id][emb_type].view(-1)\n",
    "\n",
    "      distance = torch.sqrt(torch.sum(torch.pow(torch.subtract(src, target), 2), dim=0))\n",
    "      emb_to_target_to_dist[target_id] = distance.item()\n",
    "    movieId_to_emb_type_to_similarities[movieId][emb_type] = list(sorted(emb_to_target_to_dist.items(), key=lambda item: item[1]))[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpPb5tqphOB-"
   },
   "outputs": [],
   "source": [
    "titles = [\n",
    "    'Lord of the Rings: The Return of the King, The (2003)',\n",
    "    'Star Wars: Episode IV - A New Hope (1977)',\n",
    "    'Toy Story (1995)',\n",
    "    'Saving Private Ryan (1998)',\n",
    "    'Kill Bill: Vol. 1 (2003)',\n",
    "    'American Pie (1999)',\n",
    "    'Blair Witch Project, The (1999)',\n",
    "    'Princess Mononoke (Mononoke-hime) (1997)'\n",
    "]\n",
    "\n",
    "emb_type = 'MOVIE_EMBEDDING_COMBINED'\n",
    "\n",
    "table = '| Movie |'\n",
    "for i in range(5):\n",
    "  table += ' Similar {} |'.format(i+1)\n",
    "table += '\\n'\n",
    "for i in range(5):\n",
    "  table += '|-----'\n",
    "table += '|\\n'\n",
    "\n",
    "# Print the top most similar movies\n",
    "for title in titles:\n",
    "  movieId = title_to_movieId[title]\n",
    "\n",
    "  table += '| '\n",
    "  for target_id, dist in movieId_to_emb_type_to_similarities[movieId][emb_type][0:5+1]:\n",
    "    table += movieId_to_title[target_id] + ' | '\n",
    "  table += '\\n'\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDjCGr5Lhu-K"
   },
   "source": [
    "### Get Recommendations for Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG4oRn7ztTNe"
   },
   "outputs": [],
   "source": [
    "for movieId in user_context_movies:\n",
    "    print(movieId_to_title[movieId], movieId_to_genres[movieId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukMIw_w0iLy_"
   },
   "outputs": [],
   "source": [
    "user_type_to_favorite_genres = {\n",
    "    'Fantasy Lover': ['Fantasy'],\n",
    "    'Children\\'s Movie Lover': ['Children'],\n",
    "    'Horror Lover': ['Horror'],\n",
    "    'Sci-Fi Lover': ['Sci-Fi'],\n",
    "    'Comedy Lover': ['Comedy'],\n",
    "    'Romance Lover': ['Romance'],\n",
    "    'War Movie Lover': ['War'],\n",
    "    'Old Movie Lover': [],\n",
    "\n",
    "    # profile for myself\n",
    "    'Myself': ['Fantasy', 'War', 'Horror', 'Drama', 'Action']\n",
    "}\n",
    "\n",
    "user_type_to_worst_genres = {\n",
    "    'Fantasy Lover': ['Horror', 'Children'],\n",
    "    'Children\\'s Movie Lover': ['Horror', 'Romance', 'Drama'],\n",
    "    'Horror Lover': ['Children'],\n",
    "    'Sci-Fi Lover': ['Romance', 'Children'],\n",
    "    'Comedy Lover': ['Children'],\n",
    "    'Romance Lover': ['Children', 'Horror'],\n",
    "    'War Movie Lover': ['Children'],\n",
    "    'Old Movie Lover': [],\n",
    "    \n",
    "    # profile for myself\n",
    "    'Myself': ['Romance']\n",
    "}\n",
    "\n",
    "user_type_to_favorite_movies = {\n",
    "    'Fantasy Lover': [\n",
    "        'Lord of the Rings: The Fellowship of the Ring, The (2001)',\n",
    "        'Lord of the Rings: The Two Towers, The (2002)',\n",
    "        'Gladiator (2000)',\n",
    "        '300 (2007)',\n",
    "        'Braveheart (1995)'\n",
    "        ],\n",
    "    'Children\\'s Movie Lover': [\n",
    "        'Toy Story 2 (1999)',\n",
    "        'Finding Nemo (2003)',\n",
    "        'Monsters, Inc. (2001)'\n",
    "        ],\n",
    "    'Horror Lover': [\n",
    "        'Blair Witch Project, The (1999)',\n",
    "        'Silence of the Lambs, The (1991)',\n",
    "        'Sixth Sense, The (1999)'\n",
    "        ],\n",
    "    'Sci-Fi Lover': [\n",
    "        'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
    "        'Matrix, The (1999)',\n",
    "        'Terminator, The (1984)'\n",
    "        ],\n",
    "    'Comedy Lover': [\n",
    "        'American Pie (1999)',\n",
    "        'Dumb & Dumber (Dumb and Dumber) (1994)',\n",
    "        'Austin Powers: The Spy Who Shagged Me (1999)',\n",
    "        'Big Lebowski, The (1998)'\n",
    "      ],\n",
    "    'Romance Lover': [\n",
    "        'Shakespeare in Love (1998)',\n",
    "        'There\\'s Something About Mary (1998)',\n",
    "        'Sense and Sensibility (1995)'\n",
    "    ],\n",
    "    'War Movie Lover': [\n",
    "        'Saving Private Ryan (1998)',\n",
    "        'Apocalypse Now (1979)',\n",
    "        'Full Metal Jacket (1987)'\n",
    "    ],\n",
    "    'Old Movie Lover': [\n",
    "        '12 Angry Men (1957)',\n",
    "        'Graduate, The (1967)',\n",
    "        'Wizard of Oz, The (1939)',\n",
    "        'Psycho (1960)',\n",
    "        'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)',\n",
    "    ],\n",
    "    \n",
    "    # profile for myself\n",
    "    'Myself': [\n",
    "        'Lord of the Rings: The Fellowship of the Ring, The (2001)',\n",
    "        'Lord of the Rings: The Two Towers, The (2002)',\n",
    "        'Lord of the Rings: The Return of the King, The (2003)',\n",
    "        '300 (2007)',\n",
    "        'Saving Private Ryan (1998)',\n",
    "        'Kill Bill: Vol. 1 (2003)',\n",
    "    ]\n",
    "}\n",
    "\n",
    "value_for_favorite_genre_avg_rating = float(5.0) # make much larger than usual to make liked genres have more emphasis\n",
    "value_for_disliked_genre_avg_rating = float(-2.0)\n",
    "value_for_favorite_movie_rating = float(2.0)\n",
    "\n",
    "user_to_inference_context = {}\n",
    "\n",
    "for user_type in user_type_to_favorite_genres.keys():\n",
    "  inference_user_context = [0.0] * user_context_size\n",
    "\n",
    "  # set genres the user likes (avg rating + ratio)\n",
    "  for genre in user_type_to_favorite_genres[user_type]:\n",
    "    inference_user_context[user_context_genre_avg_rating_to_i[genre]] = value_for_favorite_genre_avg_rating\n",
    "\n",
    "  # set genres that the user dislikes (avg rating)\n",
    "  for genre in user_type_to_worst_genres[user_type]:\n",
    "    inference_user_context[user_context_genre_avg_rating_to_i[genre]] = value_for_disliked_genre_avg_rating\n",
    "\n",
    "  # set the user's favorite movies.\n",
    "  for title in user_type_to_favorite_movies[user_type]:\n",
    "    movieId = title_to_movieId[title]\n",
    "    inference_user_context[user_context_movieId_to_i[movieId]] = value_for_favorite_movie_rating\n",
    "\n",
    "  user_to_inference_context[user_type] = inference_user_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzX6zAt7Wgqx"
   },
   "outputs": [],
   "source": [
    "user_to_top_recs = {}\n",
    "\n",
    "for user_type in user_to_inference_context.keys():\n",
    "\n",
    "  X_inference = torch.tensor([user_to_inference_context[user_type]])\n",
    "  user_embedding_inference = model.user_feature_tower(X_inference)\n",
    "\n",
    "  # we also need a timestamp in order to run inference, so let's use the max timestamp from all our training data\n",
    "  timestamp_inference = torch.bucketize(torch.tensor([df_ratings['timestamp'].max()]), timestamp_bins, right=False)\n",
    "  timestamp_embedding = model.timestamp_embedding_tower(model.timestamp_embedding_lookup(timestamp_inference))\n",
    "\n",
    "  # combine the user and timestamp embeddings just as we do when training\n",
    "  user_and_timestamp_combined_embedding = torch.cat((user_embedding_inference, timestamp_embedding), dim=1)\n",
    "\n",
    "  movieId_to_pred_score = {}\n",
    "  for movieId in top_movies:\n",
    "    # we already have the combined item embedding for every movie to make inference easier.\n",
    "    item_embedding_combined_inference = movieId_to_embedding[movieId]['MOVIE_EMBEDDING_COMBINED']\n",
    "    preds = torch.einsum('ij, ij -> i', user_and_timestamp_combined_embedding, item_embedding_combined_inference)\n",
    "    movieId_to_pred_score[movieId] = preds\n",
    "\n",
    "  top_recs = []\n",
    "  show_top_recs = True\n",
    "  for movieId, pred_score in list(sorted(movieId_to_pred_score.items(), key=lambda item: item[1], reverse=show_top_recs)):\n",
    "    if len(top_recs) >= 10: break\n",
    "    if movieId_to_title[movieId] not in user_type_to_favorite_movies[user_type]:\n",
    "      top_recs.append(movieId)\n",
    "  user_to_top_recs[user_type] = top_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxtcVRHzGucO",
    "outputId": "a288f48c-7049-4954-8bd0-883b4992df24"
   },
   "outputs": [],
   "source": [
    "for user_type in user_to_top_recs.keys():\n",
    "  print(\"Hello, \" + user_type)\n",
    "  print(\"Because you like: [\" + ','.join(user_type_to_favorite_genres[user_type]) + ']')\n",
    "  print(\"And hate: [\" + ','.join(user_type_to_worst_genres[user_type]) + ']')\n",
    "  print(\"And enjoyed these movies:\")\n",
    "  for title in user_type_to_favorite_movies[user_type]:\n",
    "    print(title)\n",
    "  print()\n",
    "\n",
    "  print(\"You should watch:\")\n",
    "  for movieId in user_to_top_recs[user_type]:\n",
    "    print(movieId_to_title[movieId])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_IgQlt4kEPp"
   },
   "outputs": [],
   "source": [
    "# sanity check - make sure we aren't just recommending the higest rated movies\n",
    "# NOTE: this is an extremely common problem in rec systems as the model learns\n",
    "# to play it safe and just recommend what almost is highly rated.\n",
    "for movieId, avg_rating in sorted(movieId_to_avg_rating.items(), key=lambda item: item[1], reverse=True)[0:10]:\n",
    "  print(movieId_to_title[movieId], avg_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the 'most representative' movies for a specfic GENRE\n",
    "genre = 'Romance'\n",
    "genre_context = [0.0] * len(genres)\n",
    "genre_context[genre_to_i[genre]] = float(1.0)\n",
    "genre_context_emb = model.item_genre_tower(torch.tensor([genre_context]))\n",
    "\n",
    "movieId_to_dist = {}\n",
    "for movieId in top_movies:\n",
    "  distance = torch.sqrt(torch.sum(torch.pow(torch.subtract(movieId_to_embedding[movieId]['MOVIE_GENRE_EMBEDDING'].view(-1), genre_context_emb.view(-1)), 2), dim=0))\n",
    "  movieId_to_dist[movieId] = distance.item()\n",
    "\n",
    "for movieId,dist in list(sorted(movieId_to_dist.items(), key=lambda item: item[1]))[0:10]:\n",
    "    print(movieId_to_title[movieId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_to_tag_to_count[title_to_movieId['Saw (2004)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the 'most representative' movies for a specfic TAG\n",
    "tags = ['horror', 'gore', 'torture']\n",
    "tags = ['dystopia', 'artificial intelligence', 'martial arts']\n",
    "tag_context = [0.0] * len(final_movie_tags)\n",
    "for tag in tags:\n",
    "  tag_context[tag_to_i[tag]] = float(10.0)\n",
    "tag_context_emb = model.item_tag_tower(torch.tensor([tag_context]))\n",
    "\n",
    "movieId_to_dist = {}\n",
    "for movieId in top_movies:\n",
    "  distance = torch.sqrt(torch.sum(torch.pow(torch.subtract(movieId_to_embedding[movieId]['MOVIE_TAG_EMBEDDING'].view(-1), tag_context_emb.view(-1)), 2), dim=0))\n",
    "  movieId_to_dist[movieId] = distance.item()\n",
    "\n",
    "for movieId,dist in list(sorted(movieId_to_dist.items(), key=lambda item: item[1]))[0:10]:\n",
    "    print(movieId_to_title[movieId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
